# -*- coding: utf-8 -*-
"""FA_assignment2_clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12zu0mhAujhr_L-56XXEmXQAqRaHBUWDW
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns

data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/ASSIGNMENTS/FA_assignment/multihop-outdoor-moteid1-data-converted_wo_head.csv', 
                   delimiter= ',')
print(data.head(10))
print(data.shape)
print(data.dtypes)
print(data.describe())

xtrain = data.iloc[:, 0:-1]
ytrain = data.iloc[:, -1]

data.iloc[:, -1].value_counts()

print(xtrain.columns.values)

from pandas.api.types import is_numeric_dtype

for col in xtrain.columns:
    if is_numeric_dtype(xtrain[col]):
        print('%s:' % (col))
        print('\t Mean = %.2f' % xtrain[col].mean())
        print('\t Standard deviation = %.2f' % xtrain[col].std())
        print('\t Minimum = %.2f' % xtrain[col].min())
        print('\t Maximum = %.2f' % xtrain[col].max())

plt.scatter(xtrain['Humidity'], xtrain['Tepmrature'])
plt.xlabel('Humidity')
plt.ylabel('Temperature')
plt.show()

xtrain.plot(x='Reading#', y='Humidity', kind = 'scatter')
xtrain.plot(x='Reading#', y='Tepmrature', kind = 'scatter')
xtrain.plot(x='Reading#', y='Tepmrature', kind = 'box')

xtrain.plot(x='Reading#', y='Humidity', kind = 'line')
plt.xlabel('Reading#')
plt.ylabel('Humidity')

xtrain.plot(x='Reading#', y='Tepmrature', kind = 'line')
plt.xlabel('Reading#')
plt.ylabel('Temperature')

fig = plt.figure()
ax1 = fig.add_subplot(111)

xtrain.plot.scatter(x='Reading#', y='Humidity', c='b', marker="s", label='Humidity', ax=ax1 )
xtrain.plot.scatter(x='Reading#', y='Tepmrature', c='r', marker="o", label='Temperature', ax= ax1)
plt.ylabel('Temperature and Humidity')
plt.title('Anomaly in Temperature and Humidity vs. Readings ')
plt.legend(loc='upper left')
plt.show()


n_cluster = range(1, 20)
kmeans = [KMeans(n_clusters=i).fit(xtrain) for i in n_cluster]
scores = [kmeans[i].score(xtrain) for i in range(len(kmeans))]

fig, ax = plt.subplots(figsize=(10,6))
ax.plot(n_cluster, scores)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()



# for finding the principal components
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

min_max_scaler = preprocessing.StandardScaler()
np_scaled = min_max_scaler.fit_transform(data2)

data2 = pd.DataFrame(np_scaled)

pca = PCA(n_components=2)
data = pca.fit_transform(data2)

# standardize these 2 new features
min_max_scaler = preprocessing.StandardScaler()
np_scaled = min_max_scaler.fit_transform(data)
data = pd.DataFrame(np_scaled)

# K-Means Clustering
n_cluster = range(1, 4)
kmeans = [KMeans(n_clusters=i).fit(data2) for i in n_cluster]
scores = [kmeans[i].score(data2) for i in range(len(kmeans))]
fig, ax = plt.subplots()
ax.plot(n_cluster, scores)
plt.show()

# if taking feature 0 and feature 1 as the principal components
data['cluster'] = kmeans[2].predict(data2)
data['principal_feature1'] = data[0]
data['principal_feature2'] = data[1]
data['cluster'].value_counts()


fig, ax = plt.subplots()
colors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'black', 5:'cyan'}
ax.scatter(data['principal_feature1'], data['principal_feature2'], 
           c=data["cluster"].apply(lambda x: colors[x]))
plt.show()


# the K-Means clustering algorithm 

true_labels = np.array(ytrain)
kmeans = KMeans(n_clusters=2, random_state=0)
clusters = kmeans.fit(xtrain)
kmeans.cluster_centers_.shape
labels_kmeans = clusters.labels_

print('Precision Score:', precision_score(true_labels, labels_kmeans, average='macro'))
print('Recall Score:', recall_score(true_labels, labels_kmeans, average='macro'))
print('F1 Score:', f1_score(true_labels, labels_kmeans, average='macro'))



# implementation of the DBSCAN algorithm
# DBSCAN clustering algorithms
import numpy as np

from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.datasets.samples_generator import make_blobs
from sklearn.preprocessing import StandardScaler

# Compute DBSCAN
db = DBSCAN(eps=0.3, min_samples=10).fit(X)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels_dbscan = db.labels_

# clust3 = DBSCAN(eps=7, min_samples=4)
# y_dbscan = clust3.fit(xtrain)
# # t0 = time.time()
# labels_dbscan = clust3.labels_

from sklearn.metrics import accuracy_score
accuracy_score(labels_dbscan, ytrain)

print('Precision Score:', precision_score(true_labels, labels_dbscan, average='macro'))
print('Recall Score:', recall_score(true_labels, labels_dbscan, average='macro'))
print('F1 Score:', f1_score(true_labels, labels_dbscan, average='macro'))

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)

print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)
print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))
print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))
print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))
print("Adjusted Rand Index: %0.3f"
#       % metrics.adjusted_rand_score(labels_true, labels))
print("Adjusted Mutual Information: %0.3f"
#       % metrics.adjusted_mutual_info_score(labels_true, labels))
print("Silhouette Coefficient: %0.3f"
#       % metrics.silhouette_score(labels_true, labels))


# Plot result
import matplotlib.pyplot as plt

# Black removed and is used for noise instead.
unique_labels = set(labels)
colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise.
        col = [0, 0, 0, 1]

    class_member_mask = (labels == k)

    xy = X[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 3], 'o', markerfacecolor=tuple(col),
             markeredgecolor='b', markersize=10)

    xy = X[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 3], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=4)

plt.title('DBSCAN: Estimated number of clusters: %d' % n_clusters_)
plt.show()

from google.colab import drive
drive.mount('/content/gdrive')

data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/ASSIGNMENTS/FA_assignment/multihop-outdoor-moteid1-data-converted_wo_head.csv', 
                   delimiter= ',')
print(data.head(10))
print(data.shape)
print(data.dtypes)
print(data.describe())

xtrain = data.iloc[:, 0:-1]
xtrain.dtypes

xtrain = data.iloc[:, 0:-1]
ytrain = data.iloc[:, -1]

data.iloc[:, -1].value_counts()

print(xtrain.columns.values)

plt.scatter(xtrain['Humidity'], xtrain['Tepmrature'])
plt.xlabel('Humidity')
plt.ylabel('Temperature')
plt.show()

ax1 = xtrain.plot(kind='scatter', x='Humidity', y='Tepmrature')

xtrain.plot(x='Reading#', y='Humidity', kind = 'scatter')

xtrain.plot(x='Reading#', y='Tepmrature', kind = 'scatter')

fig = plt.figure()
ax1 = fig.add_subplot(111)

# ax1.scatter(x[:4], y[:4], s=10, c='b', marker="s", label='first')
# ax1.scatter(x[40:],y[40:], s=10, c='r', marker="o", label='second')
# plt.legend(loc='upper left');
# plt.show()


xtrain.plot.scatter(x='Reading#', y='Humidity', c='b', marker="s", label='Humidity', ax=ax1 )
xtrain.plot.scatter(x='Reading#', y='Tepmrature', c='r', marker="o", label='Temperature', ax= ax1)
plt.ylabel('Temperature and Humidity')
plt.title('Anomaly in Temperature and Humidity vs. Readings ')
plt.legend(loc='upper left')
plt.show()

xtrain.plot(x='Reading#', y='Tepmrature', kind = 'box')

xtrain.plot(x='Reading#', y='Humidity', kind = 'box')

xtrain.plot(x='Reading#', y='Humidity', kind = 'line')
plt.xlabel('Reading#')
plt.ylabel('Humidity')

xtrain.plot(x='Reading#', y='Tepmrature', kind = 'line')
plt.xlabel('Reading#')
plt.ylabel('Temperature')

# data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]
n_cluster = range(1, 20)
kmeans = [KMeans(n_clusters=i).fit(xtrain) for i in n_cluster]
scores = [kmeans[i].score(xtrain) for i in range(len(kmeans))]

fig, ax = plt.subplots(figsize=(10,6))
ax.plot(n_cluster, scores)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show();

# data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]

from sklearn.preprocessing import StandardScaler


# xtrain = xtrain.values
X_std = StandardScaler().fit_transform(xtrain)
mean_vec = np.mean(X_std, axis=0)
cov_mat = np.cov(X_std.T)
eig_vals, eig_vecs = np.linalg.eig(cov_mat)
eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]
eig_pairs.sort(key = lambda x: x[0], reverse= True)
tot = sum(eig_vals)
var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance
cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance

plt.figure(figsize=(10, 5))
plt.bar(range(len(var_exp)), var_exp, alpha=0.3, align='center', label='Individual variance', color = 'g')
plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='Cumulative variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.legend(loc='best')
plt.show()

# def getDistanceByPoint(data, model):
#     distance = pd.Series()
#     for i in range(0,len(data)):
#         Xa = np.array(data[i])
#         Xb = model.cluster_centers_[model.labels_[i]-1]
#         distance.set_value(i, np.linalg.norm(Xa-Xb))
#     return distance

# outliers_fraction = 0.01

# # get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly
# distance = getDistanceByPoint(xtrain, kmeans[5])
# number_of_outliers = int(outliers_fraction*len(distance))
# threshold = distance.nlargest(number_of_outliers).min()
# # anomaly1 contain the anomaly result of the above method Cluster (0:normal, 1:anomaly) 
# xtrain1['anomaly1'] = (distance >= threshold).astype(int)

# # visualisation of anomaly with cluster view
# fig, ax = plt.subplots(figsize=(10,6))
# colors = {0:'blue', 1:'red'}
# ax.scatter(xtrain1['principal_feature1'], xtrain1['principal_feature2'], c=xtrain1["anomaly1"].apply(lambda x: colors[x]))
# plt.xlabel('principal feature1')
# plt.ylabel('principal feature2')
# plt.show();

print(ytrain.dtypes)

xtrain1 = data.iloc[:, 0:-1]
xtrain1.isna().count()

ytrain.isna().count()
ytrain.isna().sum()

xtrain1.isna().sum()

# xtrain[['Humidity']].groupby(['Tepmrature'], as_index=False).mean().sort_values(by='Tepmrature', ascending=False)

# import seaborn as sns

# g = sns.FacetGrid(xtrain, col='Humidity')
# g.map(plt.hist, 'Tepmrature', bins=30)

from pandas.api.types import is_numeric_dtype

for col in xtrain.columns:
    if is_numeric_dtype(xtrain[col]):
        print('%s:' % (col))
        print('\t Mean = %.2f' % xtrain[col].mean())
        print('\t Standard deviation = %.2f' % xtrain[col].std())
        print('\t Minimum = %.2f' % xtrain[col].min())
        print('\t Maximum = %.2f' % xtrain[col].max())

import matplotlib.pyplot as plt
xtrain.hist(figsize=(10, 10))
plt.show()

# fig, axes = plt.subplots(3, 2, figsize= (20,40))
# index = 0
# for i in range(3):
#   for j in range(i+1, 4):
#     ax1 = int(index/2)
#     ax2 = index%2
#     axes[ax1][ax2].scatter(xtrain[xtrain.columns[i]], xtrain[xtrain.columns[j]], color='red')
#     axes[ax1][ax2].set_xlabel(xtrain.column[i])
#     axes[ax1][ax2].set_ylabel(xtrain.column[j])
#     index = index + 1

# kmeans = KMeans(n_clusters=10, random_state=0)
# clusters = kmeans.fit_predict(digits.data)
# kmeans.cluster_centers_.shape
kmeans = KMeans(n_clusters=4, random_state=0) 
y_predictions = kmeans.fit(xtrain)

centres = kmeans.cluster_centers_

plt.scatter(xtrain.iloc[:, 0], xtrain.iloc[:, 1], c=y_predictions, s=50)

centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)

from sklearn.datasets.samples_generator import make_blobs
xtrain1, ytrain1 = make_blobs(n_samples=300, centers=4,
                            cluster_std = 0.60, random_state =0)
plt.scatter(xtrain1[:, 0], xtrain1[:,1], s=50)

kmeans = KMeans(n_clusters=4) # You want cluster the passenger records into 2: Survived or Not survived
kmeans.fit(xtrain1)

y_kmeans = kmeans.predict(xtrain1)

from sklearn.metrics import confusion_matrix
mat = confusion_matrix(digits.target, labels)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,
            xticklabels=digits.target_names,
            yticklabels=digits.target_names)
plt.xlabel('true label')
plt.ylabel('predicted label');

plt.scatter(xtrain1[:, 0], xtrain1[:, 1], c=y_kmeans, s=50)

centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)

from sklearn.cluster import KMeans
est = KMeans(4)  # 4 clusters
est.fit(xtrain1)
y_kmeans = est.predict(xtrain1)
plt.scatter(xtrain1[:, 0], xtrain1[:, 1], c=y_kmeans, s=50, cmap='rainbow');

from scipy.stats import mode

labels = np.zeros_like(y_kmeans)
for i in range(4):
    mask = (y_kmeans == i)
    labels[mask] = mode(ytrain1[mask])[0]

from sklearn.metrics import accuracy_score
accuracy_score(ytrain1, labels)

from fig_code import plot_kmeans_interactive
plot_kmeans_interactive();

from sklearn.metrics import pairwise_distances_argmin

def find_clusters(xtrain, n_clusters, rseed=2):
  
  rng =np.random.RandomState(rseed)
  i = rng.permutation(xtrain[0])[:n_clusters]
  centers = xtrain[i]
  
  while True:
    labels = pairwise_distances_argmin(xtrain, centers)
    
    new_centers = np.array([xtrain[labels == i].mean(0)
                          for i in range(n_clusters)])
    
    if np.all(centers == new_centers):
      break
    centers = new_centers
    
    
  return centers, labels



centers, labels = find_clusters(xtrain, 2, rseed=2)
plt.scatter(xtrain[:, 0], xtrain[:, 1], c = labels,
            s=50)

correct = 0
for i in range(len(xtrain)):
    predict_me = np.array(xtrain[i].astype(float))
    predict_me = predict_me.reshape(-1, len(predict_me))
    prediction = kmeans.predict(predict_me)
    if prediction[0] == ytrain[i]:Labelled Data Collection for Anomaly Detection in Wireless Sensor Networks
        correct += 1

print(correct/len(xtrain))

# import pandas as pd
# import matplotlib.pyplot as plt
# import numpy as np

# x = np.linspace(0,6.3, 50)
# a = (np.sin(x)+1)*3
# b = (np.cos(x)+1)*3
# c = np.ones_like(x)*3
# d = np.exp(x)/100.
# df = pd.DataFrame({"x":x, "a":a, "b":b, "c":c, "d":d})

# ax = df.plot(kind="scatter", x="x",y="a", color="b", label="a vs. x")
# df.plot(x="x",y="b", color="r", label="b vs. x", ax=ax)
# df.plot( x="x",y="c", color="g", label="c vs. x", ax=ax)
# df.plot( x="d",y="x", color="orange", label="b vs. d", ax=ax)
# df.plot( x="a",y="x", color="purple", label="x vs. a", ax=ax)

# ax.set_xlabel("horizontal label")
# ax.set_ylabel("vertical label")
# plt.show()

from sklearn.cluster import KMeans

for i in range(1,2):

# DBSCAN clustering algorithms
import numpy as np

from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.datasets.samples_generator import make_blobs
from sklearn.preprocessing import StandardScaler

# X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,
#                             random_state=0)
X, labels_true = xtrain, ytrain
X = StandardScaler().fit_transform(X)


# Compute DBSCAN
db = DBSCAN(eps=0.3, min_samples=10).fit(X)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)

print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)
print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))
print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))
print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))
print("Adjusted Rand Index: %0.3f"
#       % metrics.adjusted_rand_score(labels_true, labels))
print("Adjusted Mutual Information: %0.3f"
#       % metrics.adjusted_mutual_info_score(labels_true, labels))
print("Silhouette Coefficient: %0.3f"
#       % metrics.silhouette_score(X, labels))


# Plot result
import matplotlib.pyplot as plt

# Black removed and is used for noise instead.
unique_labels = set(labels)
colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise.
        col = [0, 0, 0, 1]

    class_member_mask = (labels == k)

    xy = X[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 2], xy[:, 3], 'o', markerfacecolor=tuple(col),
             markeredgecolor='b', markersize=10)

    xy = X[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 2], xy[:, 3], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=6)

plt.title('DBSCAN: Estimated number of clusters: %d' % n_clusters_)
plt.show()

# DBSCAN clustering algorithms
import numpy as np

from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.datasets.samples_generator import make_blobs
from sklearn.preprocessing import StandardScaler

# X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,
#                             random_state=0)
X, labels_true = xtrain, ytrain
X = StandardScaler().fit_transform(X)


# Compute DBSCAN
db = DBSCAN(eps=0.3, min_samples=10).fit(X)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)

print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)
print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))
print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))
print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))
print("Adjusted Rand Index: %0.3f"
#       % metrics.adjusted_rand_score(labels_true, labels))
print("Adjusted Mutual Information: %0.3f"
#       % metrics.adjusted_mutual_info_score(labels_true, labels))
print("Silhouette Coefficient: %0.3f"
#       % metrics.silhouette_score(X, labels))


# Plot result
import matplotlib.pyplot as plt

# Black removed and is used for noise instead.
unique_labels = set(labels)
colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise.
        col = [0, 0, 0, 1]

    class_member_mask = (labels == k)

    xy = X[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 3], 'o', markerfacecolor=tuple(col),
             markeredgecolor='b', markersize=10)

    xy = X[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 3], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=4)

plt.title('DBSCAN: Estimated number of clusters: %d' % n_clusters_)
plt.show()

import numpy as np
import matplotlib.pyplot as plt

size = 256,16
dpi = 72.0
figsize= size[0] / float(dpi), size[1] / float(dpi)
fig = plt.figure(figsize=figsize, dpi=dpi)
fig.patch.set_alpha(0)
plt.axes([0, 0, 1, 1], frameon=False)

for i in range(1, 11):
    r, g, b = np.random.uniform(0, 1, 3)
    plt.plot([i, ], [1, ], 's', markersize=5, markerfacecolor='w',
             markeredgewidth=1.5, markeredgecolor=(r, g, b, 1))

plt.xlim(0, 11)
plt.xticks(())
plt.yticks(())

plt.show()

# DBSCAN clustering algorithms
import numpy as np

from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.datasets.samples_generator import make_blobs
from sklearn.preprocessing import StandardScaler

# X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,
#                             random_state=0)
X, labels_true = xtrain, ytrain
X = StandardScaler().fit_transform(X)


# Compute DBSCAN
db = DBSCAN(eps=0.5, min_samples=10).fit(X)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)

print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)
print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))
print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))
print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))
print("Adjusted Rand Index: %0.3f"
#       % metrics.adjusted_rand_score(labels_true, labels))
print("Adjusted Mutual Information: %0.3f"
#       % metrics.adjusted_mutual_info_score(labels_true, labels))
print("Silhouette Coefficient: %0.3f"
#       % metrics.silhouette_score(X, labels))


# Plot result
import matplotlib.pyplot as plt

# Black removed and is used for noise instead.
unique_labels = set(labels)
colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise.
        col = [0, 0, 0, 1]

    class_member_mask = (labels == k)

    xy = X[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 3], 'o', markerfacecolor=tuple(col),
             markeredgecolor='b', markersize=10)

    xy = X[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 3], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=4)

plt.title('DBSCAN: Estimated number of clusters: %d' % n_clusters_)
plt.show()

