# -*- coding: utf-8 -*-
"""fraud_assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XHtc_5VmPxVbOTH5f3bDnmeZg9I9bKjR
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns

from google.colab import drive
drive.mount('/content/gdrive')

data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/ASSIGNMENTS/FA_assignment/multihop-outdoor-moteid1-data-converted_wo_head.csv', 
                   delimiter= ',')
print(data.head(10))

xtrain = data.iloc[:, 0:-1]
ytrain = data.iloc[:, -1]

true_labels = np.array(ytrain)

kmeans = KMeans(n_clusters=4, random_state=0)
clusters = kmeans.fit_predict(xtrain)
kmeans.cluster_centers_.shape



kmns = KMeans(n_clusters=4, random_state=0).fit(xtrain)
core_samples_mask = np.zeros_like(kmns.labels_, dtype=bool)
# core_samples_mask[kmns.core_sample_indices_] = True
labels_kmns = kmns.labels_

labels_kmeans = np.zeros_like(clusters)

from sklearn.metrics import accuracy_score
accuracy_score(labels_kmeans, ytrain)

plt.scatter(xtrain.iloc[:,0], xtrain.iloc[:,3],c=labels_kmeans, cmap='Paired')
plt.title("K-means")

# from sklearn.metrics import confusion_matrix
# mat = confusion_matrix(labels_kmeans, ytrain)
# sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,
#             xticklabels=labels_kmeans,
#             yticklabels=ytrain)
# plt.xlabel('true label')
# plt.ylabel('predicted label');

from sklearn.metrics import confusion_matrix
confusion_matrix(ytrain, labels_kmeans)

from sklearn.metrics import f1_score
f1_score(ytrain, labels_kmeans, average='macro')

from sklearn.metrics import average_precision_score
average_precision_score(ytrain, labels_kmeans)

from sklearn.metrics import precision_recall_curve
precision, recall, _ = precision_recall_curve(ytrain, labels_kmeans)
print(precision, recall)

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(ytrain, labels_kmeans)
plt.imshow(cm,interpolation='none',cmap='Blues')

for (i, j), z in np.ndenumerate(cm):
    plt.text(j, i, z, ha='center', va='center')
    
plt.xlabel("kmeans label")
plt.ylabel("truth label")
plt.show()

from sklearn import metrics

true_labels = np.array(ytrain)

ARI = metrics.adjusted_rand_score(true_labels, labels_kmeans) 

MI = metrics.adjusted_mutual_info_score(true_labels, labels_kmeans)

print(ARI, MI)

# from sklearn.cluster import OPTICS, cluster_optics_dbscan

# clust2 = OPTICS(min_samples=50, xi=.05, min_cluster_size=.05)

# # Run the fit
# clust2.fit(xtrain)

data2 = data



from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

min_max_scaler = preprocessing.StandardScaler()
np_scaled = min_max_scaler.fit_transform(data2)

data2 = pd.DataFrame(np_scaled)

pca = PCA(n_components=2)
data = pca.fit_transform(data2)

# standardize these 2 new features
min_max_scaler = preprocessing.StandardScaler()
np_scaled = min_max_scaler.fit_transform(data)
data = pd.DataFrame(np_scaled)

n_cluster = range(1, 6)
kmeans = [KMeans(n_clusters=i).fit(data2) for i in n_cluster]
scores = [kmeans[i].score(data2) for i in range(len(kmeans))]
fig, ax = plt.subplots()
ax.plot(n_cluster, scores)
plt.show()

# if taking feature 0 and feature 1 as the principal components
data['cluster'] = kmeans[4].predict(data2)
data['principal_feature1'] = data[0]
data['principal_feature2'] = data[1]
data['cluster'].value_counts()

fig, ax = plt.subplots()
colors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'black'}
ax.scatter(data['principal_feature1'], data['principal_feature2'], 
           c=data["cluster"].apply(lambda x: colors[x]))
plt.show()

data['cluster'] = kmeans[4].predict(data2)
data['principal_feature1'] = data[0]
data['principal_feature2'] = data[1]
data['cluster'].value_counts()

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

min_max_scaler = preprocessing.StandardScaler()
np_scaled = min_max_scaler.fit_transform(data2)

data2 = pd.DataFrame(np_scaled)

pca = PCA(n_components=2)
data = pca.fit_transform(data2)

# standardize these 2 new features
min_max_scaler = preprocessing.StandardScaler()
np_scaled = min_max_scaler.fit_transform(data)
data = pd.DataFrame(np_scaled)

n_cluster = range(1, 5)
kmeans = [KMeans(n_clusters=i).fit(data2) for i in n_cluster]
scores = [kmeans[i].score(data2) for i in range(len(kmeans))]
fig, ax = plt.subplots()
ax.plot(n_cluster, scores)
plt.show()

# if taking feature 0 and feature 1 as the principal components
data['cluster'] = kmeans[3].predict(data2)
data['principal_feature1'] = data[0]
data['principal_feature2'] = data[1]
data['cluster'].value_counts()


fig, ax = plt.subplots()
colors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'black'}
ax.scatter(data['principal_feature1'], data['principal_feature2'], 
           c=data["cluster"].apply(lambda x: colors[x]))
plt.show()

outliers_fraction = 0.3


def getDistanceByPoint(data, model):
    distance = pd.Series()
    for i in range(0,len(data)):
        Xa = np.array(data.loc[i])
        Xb = model.cluster_centers_[model.labels_[i]-1]
        distance.set_value(i, np.linalg.norm(Xa-Xb))
    return distance



distance = getDistanceByPoint(data2, kmeans[3])
number_of_outliers = int(outliers_fraction*len(distance))
threshold = distance.nlargest(number_of_outliers).min()
# anomaly21 contain the anomaly result of method 2.1 Cluster (0:normal, 1:anomaly) 
data['anomaly21'] = (distance >= threshold).astype(int)


fig, ax = plt.subplots()
colors = {0:'blue', 1:'red'}
ax.scatter(data['principal_feature1'], data['principal_feature2'], c=data["anomaly21"].apply(lambda x: colors[x]))
plt.show()



from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

min_max_scaler = preprocessing.StandardScaler()
np_scaled = min_max_scaler.fit_transform(data2)

data2 = pd.DataFrame(np_scaled)

pca = PCA(n_components=2)
data = pca.fit_transform(data2)

# standardize these 2 new features
min_max_scaler = preprocessing.StandardScaler()
np_scaled = min_max_scaler.fit_transform(data)
data = pd.DataFrame(np_scaled)


n_cluster = range(1, 7)
kmeans = [KMeans(n_clusters=i).fit(data2) for i in n_cluster]
scores = [kmeans[i].score(data2) for i in range(len(kmeans))]
fig, ax = plt.subplots()
ax.plot(n_cluster, scores)
plt.show()

# if taking feature 0 and feature 1 as the principal components
data['cluster'] = kmeans[5].predict(data2)
data['principal_feature1'] = data[0]
data['principal_feature2'] = data[1]
data['cluster'].value_counts()


fig, ax = plt.subplots()
colors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'yellow', 5:'cyan', 6:'purple'}
ax.scatter(data['principal_feature1'], data['principal_feature2'], 
           c=data["cluster"].apply(lambda x: colors[x]))
plt.show()

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

min_max_scaler = preprocessing.StandardScaler()
np_scaled = min_max_scaler.fit_transform(data2)

data2 = pd.DataFrame(np_scaled)

pca = PCA(n_components=2)
data = pca.fit_transform(data2)

# standardize these 2 new features
min_max_scaler = preprocessing.StandardScaler()
np_scaled = min_max_scaler.fit_transform(data)
data = pd.DataFrame(np_scaled)

n_cluster = range(1, 6)
kmeans = [KMeans(n_clusters=i).fit(data2) for i in n_cluster]
scores = [kmeans[i].score(data2) for i in range(len(kmeans))]
fig, ax = plt.subplots()
ax.plot(n_cluster, scores)
plt.show()

# if taking feature 0 and feature 1 as the principal components
data['cluster'] = kmeans[4].predict(data2)
data['principal_feature1'] = data[0]
data['principal_feature2'] = data[1]
data['cluster'].value_counts()


fig, ax = plt.subplots()
colors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'black', 5:'cyan'}
ax.scatter(data['principal_feature1'], data['principal_feature2'], 
           c=data["cluster"].apply(lambda x: colors[x]))
plt.show()

from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

min_max_scaler = preprocessing.StandardScaler()
np_scaled = min_max_scaler.fit_transform(data2)

data2 = pd.DataFrame(np_scaled)

pca = PCA(n_components=2)
data = pca.fit_transform(data2)

# standardize these 2 new features
min_max_scaler = preprocessing.StandardScaler()
np_scaled = min_max_scaler.fit_transform(data)
data = pd.DataFrame(np_scaled)

n_cluster = range(1, 4)
kmeans = [KMeans(n_clusters=i).fit(data2) for i in n_cluster]
scores = [kmeans[i].score(data2) for i in range(len(kmeans))]
fig, ax = plt.subplots()
ax.plot(n_cluster, scores)
plt.show()

# if taking feature 0 and feature 1 as the principal components
data['cluster'] = kmeans[2].predict(data2)
data['principal_feature1'] = data[0]
data['principal_feature2'] = data[1]
data['cluster'].value_counts()


fig, ax = plt.subplots()
colors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'black', 5:'cyan'}
ax.scatter(data['principal_feature1'], data['principal_feature2'], 
           c=data["cluster"].apply(lambda x: colors[x]))
plt.show()



# labels_kmeans = np.zeros_like(labels_kmeans)

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels_kmns)) - (1 if -1 in labels else 0)
n_noise_ = list(labels_kmns).count(-1)

print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)
print("Homogeneity: %0.3f" % metrics.homogeneity_score(true_labels, labels_kmns))
print("Completeness: %0.3f" % metrics.completeness_score(true_labels, labels_kmns))
print("V-measure: %0.3f" % metrics.v_measure_score(true_labels, labels_kmns))
print("Adjusted Rand Index: %0.3f"
#       % metrics.adjusted_rand_score(true_labels, labels_kmns))
print("Adjusted Mutual Information: %0.3f"
#       % metrics.adjusted_mutual_info_score(true_labels, labels_kmns))
print("Silhouette Coefficient: %0.3f"
#       % metrics.silhouette_score(xtrain, labels_kmns))

from sklearn.cluster import DBSCAN
import time

t0 = time.time()

clust3 = DBSCAN(eps=3, min_samples=4)
y_dbscan = clust3.fit(xtrain)
t0 = time.time()
labels_dbscan = clust3.labels_

from sklearn.metrics import accuracy_score
accuracy_score(labels_dbscan, ytrain)

plt.scatter(xtrain.iloc[:,2], xtrain.iloc[:,3],c=labels_dbscan, cmap='Paired')
plt.title("DBSCAN")

from sklearn.metrics import classification_report

print(classification_report(true_labels, labels_dbscan))

from sklearn.metrics import precision_recall_fscore_support

precision_recall_fscore_support(true_labels, labels_dbscan, average='macro')

from sklearn.metrics import f1_score
f1_score(true_labels, labels_dbscan, average='macro')

from sklearn.metrics import average_precision_score
average_precision_score(true_labels, labels_dbscan)

from  sklearn.cluster import Birch
from sklearn import cluster

birch = Birch(branching_factor=50, n_clusters=None, threshold=0.5, 
            compute_labels=True)
y_brc = birch.fit(xtrain)
labels_birch= birch.predict(xtrain)
# labels_birch = birch.labels_

from sklearn.metrics import accuracy_score
accuracy_score(labels_birch, ytrain)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.cluster import KMeans
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns
from sklearn.metrics import f1_score, precision_score, recall_score


data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/ASSIGNMENTS/FA_assignment/multihop-outdoor-moteid1-data-converted_wo_head.csv', 
                   delimiter= ',')
print(data.head(10))


xtrain = data.iloc[:, 0:-1]
ytrain = data.iloc[:, -1]

true_labels = np.array(ytrain)
kmeans = KMeans(n_clusters=2, random_state=0)
clusters = kmeans.fit(xtrain)
kmeans.cluster_centers_.shape
labels_kmeans = clusters.labels_

print('Precision Score:', precision_score(true_labels, labels_kmeans, average='macro'))
print('Recall Score:', recall_score(true_labels, labels_kmeans, average='macro'))
print('F1 Score:', f1_score(true_labels, labels_kmeans, average='macro'))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/ASSIGNMENTS/FA_assignment/multihop-outdoor-moteid1-data-converted_wo_head.csv', 
                   delimiter= ',')
print(data.head(10))


xtrain = data.iloc[:, 0:-1]
ytrain = data.iloc[:, -1]

true_labels = np.array(ytrain)


from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.cluster import DBSCAN
import time

# t0 = time.time()

clust3 = DBSCAN(eps=7, min_samples=4)
y_dbscan = clust3.fit(xtrain)
# t0 = time.time()
labels_dbscan = clust3.labels_

from sklearn.metrics import accuracy_score
accuracy_score(labels_dbscan, ytrain)

print('Precision Score:', precision_score(true_labels, labels_dbscan, average='macro'))
print('Recall Score:', recall_score(true_labels, labels_dbscan, average='macro'))
print('F1 Score:', f1_score(true_labels, labels_dbscan, average='macro'))